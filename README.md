## Interesting Article:
THE famous ["Attention is all you need"](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf ). I've been reading this article for some time, probably almost a week, as I know nothing about NLP/LLM before! 
## Citations: 
1. In this work, we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.

2. Multi-Head Attention allows the model to jointly attend to information from different representation subspaces at different positions.
